{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepFake vs Real\n",
    "#### Exploring if there are detectable differences between a Deepfake and real Video by comparing structure similarity scores between sequential frames\n",
    "by Thomas Karba (310)247-9541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\thoma\\OneDrive\\Desktop\\my DS work\\Queen')\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create images from Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture('real.mkv')\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "while success:\n",
    "  cv2.imwrite(\"frame%d.jpg\" % count, image)     # save frame as JPEG file      \n",
    "  success,image = vidcap.read()\n",
    "  count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "move frames to folder in directory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture('deepfake.mp4')\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "while success:\n",
    "  cv2.imwrite(\"frame%d.jpg\" % count, image)     # save frame as JPEG file      \n",
    "  success,image = vidcap.read()\n",
    "  count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "move frames to folder in directory..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop Images to only include Face x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Image Paths list, Change directory to proper location for both Video Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "path = r'C:\\Users\\thoma\\OneDrive\\Desktop\\my DS work\\Queen\\real_frames'\n",
    "files = listdir(path)\n",
    "image_list = []\n",
    "for image in files:\n",
    "    total_path = \"\\\\\".join([path,image])\n",
    "    image_list.append(total_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Deep Neural Net model to crop image to only face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNetFromCaffe(r\"C:\\Users\\thoma\\OneDrive\\Desktop\\Sites\\CallTextMe\\FaceDetection\\master\\fd\\deploy.prototxt.txt\",\n",
    "                               r\"C:\\Users\\thoma\\OneDrive\\Desktop\\Sites\\CallTextMe\\FaceDetection\\master\\fd\\res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "count=0\n",
    "for image in image_list:\n",
    "    image = cv2.imread(image)\n",
    "    (h, w) = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0,(300, 300), (104.0, 177.0, 123.0))\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    #for i in range(0, detections.shape[2]):\n",
    "    confidence = detections[0, 0, 0, 2]\n",
    "    if confidence < .25:\n",
    "        count += 1\n",
    "        continue\n",
    "    else:\n",
    "        box = detections[0,0,0,3:7]*np.array([w,h,w,h])\n",
    "        (startx,starty,endx,endy)=box.astype(\"int\")\n",
    "        crop_img = image[starty:endy, startx:endx]\n",
    "        cv2.imwrite(\"cropped%d.jpg\" % count, crop_img)\n",
    "        count += 1\n",
    "        #cv2.imshow(\"cropped\",crop_img)\n",
    "        #cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for loop to grayscale and compute structural similarity for 1 pair of sequential images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(133, 99) (130, 99)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# load the two input images\n",
    "imageA = cv2.imread(r\"C:\\Users\\thoma\\OneDrive\\Desktop\\my DS work\\Queen\\deepfake_crop\\cropped4.jpg\")\n",
    "imageB = cv2.imread(r\"C:\\Users\\thoma\\OneDrive\\Desktop\\my DS work\\Queen\\deepfake_crop\\cropped5.jpg\")\n",
    "# convert the images to grayscale\n",
    "grayA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\n",
    "grayB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n",
    "print(grayA.shape,grayB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM: 0.8745263587821754\n"
     ]
    }
   ],
   "source": [
    "from skimage.metrics import structural_similarity as ss\n",
    "\n",
    "(score, diff) = ss(grayA[:130,:], grayB, full=True)\n",
    "diff = (diff * 255).astype(\"uint8\")\n",
    "print(\"SSIM: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list with paths to all the cropped faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "path = r'C:\\Users\\thoma\\OneDrive\\Desktop\\my DS work\\Queen\\deepfake_crop'\n",
    "files = listdir(path)\n",
    "image_list = []\n",
    "for image in files:\n",
    "    total_path = \"\\\\\".join([path,image])\n",
    "    image_list.append(total_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the minimum image sizes in the cropped faces for the structural similarity function, which requires images to be the same size. The images will be sliced to fit one another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(data={\"A\":[None],\"B\":[None]})\n",
    "for image in image_list:\n",
    "    read = cv2.imread(image)\n",
    "    gray = cv2.cvtColor(read, cv2.COLOR_BGR2GRAY)\n",
    "    a,b = gray.shape\n",
    "    df = df.append({\"A\":a,\"B\":b},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    116.0\n",
       "B     96.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"A\",\"B\"]].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and create a datframe of scores, from one image to the next for the deepfake video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from skimage.metrics import structural_similarity as ss\n",
    "\n",
    "scores=pd.DataFrame(data={\"SSIM\":[None]})\n",
    "for i in range(0,len(image_list)):\n",
    "    pathA = image_list[i]\n",
    "    try:\n",
    "        pathB = image_list[i+1]\n",
    "    except IndexError:\n",
    "        break\n",
    "    imageA = cv2.imread(pathA)\n",
    "    imageB = cv2.imread(pathB)\n",
    "    grayA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\n",
    "    grayB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n",
    "    (score, diff) = ss(grayA[:116,:96], grayB[:116,:96], full=True)\n",
    "    #diff = (diff * 255).astype(\"uint8\")\n",
    "    scores = scores.append({\"SSIM\":score},ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    883.000000\n",
       "mean       0.809946\n",
       "std        0.229199\n",
       "min        0.249284\n",
       "25%        0.782197\n",
       "50%        0.916632\n",
       "75%        0.967253\n",
       "max        1.000000\n",
       "Name: SSIM, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['SSIM'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the minimum image dimensions for all the cropped real face images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    81.0\n",
       "B    67.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "path = r'C:\\Users\\thoma\\OneDrive\\Desktop\\my DS work\\Queen\\real_crop'\n",
    "files = listdir(path)\n",
    "image_list = []\n",
    "for image in files:\n",
    "    total_path = \"\\\\\".join([path,image])\n",
    "    image_list.append(total_path)\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(data={\"A\":[None],\"B\":[None]})\n",
    "for image in image_list:\n",
    "    read = cv2.imread(image)\n",
    "    gray = cv2.cvtColor(read, cv2.COLOR_BGR2GRAY)\n",
    "    a,b = gray.shape\n",
    "    df = df.append({\"A\":a,\"B\":b},ignore_index=True)\n",
    "df[[\"A\",\"B\"]].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and create a datframe of scores, from one image to the next for the real video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from skimage.metrics import structural_similarity as ss\n",
    "\n",
    "scores_real=pd.DataFrame(data={\"SSIM\":[None]})\n",
    "for i in range(0,len(image_list)):\n",
    "    pathA = image_list[i]\n",
    "    try:\n",
    "        pathB = image_list[i+1]\n",
    "    except IndexError:\n",
    "        break\n",
    "    imageA = cv2.imread(pathA)\n",
    "    imageB = cv2.imread(pathB)\n",
    "    grayA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\n",
    "    grayB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n",
    "    (score, diff) = ss(grayA[:81,:67], grayB[:81,:67], full=True)\n",
    "    #diff = (diff * 255).astype(\"uint8\")\n",
    "    scores_real = scores_real.append({\"SSIM\":score},ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare the Structural Similarity scores of both videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    883.000000\n",
       "mean       0.809946\n",
       "std        0.229199\n",
       "min        0.249284\n",
       "25%        0.782197\n",
       "50%        0.916632\n",
       "75%        0.967253\n",
       "max        1.000000\n",
       "Name: SSIM, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['SSIM'].describe() # DEEPFAKE of QUEEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    839.000000\n",
       "mean       0.849257\n",
       "std        0.126434\n",
       "min        0.523660\n",
       "25%        0.719828\n",
       "50%        0.906436\n",
       "75%        0.952260\n",
       "max        1.000000\n",
       "Name: SSIM, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_real['SSIM'].describe() # REAL QUEEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the Videos have a different length of frames, \n",
    "both dataframes will be limited to a similar size in order\n",
    "to do a T-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((839,), (839,))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake,real = scores[1:840]['SSIM'].values,scores_real[1:]['SSIM'].values\n",
    "\n",
    "fake.shape,real.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the scores for both videos as to compare the approximate Variance between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Real:  Seq. Similarity')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(fake)\n",
    "plt.ylim(0,1.1)\n",
    "plt.title(\"DeepFake: Seq. Similarity\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(real)\n",
    "plt.ylim(0,1.1)\n",
    "plt.title(\"Real:  Seq. Similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many more outliers in the DeepFake scores, as well as a tighter distribution as evident by the differences between the 25th and 75th percentile\n",
    "\n",
    "Perform a T-test with equal Variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-4.548219206286324, pvalue=5.798614320572615e-06)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "stats.ttest_ind(fake,real,equal_var=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *There is sufficient evidence to confirm there is statistically significant difference between the mean structure similary of the DeepFake Queen video and the structure similarity of the Real Queen video (p-value:5.7e-6 from T-test).*\n",
    "\n",
    "This confirms there are detectable differences between a real video of a face and a computer generated video of a face. \n",
    "The scope of this project is limited however to just two videos or one pair - using Queen Elisabeth deepfake and a real video. More sample-pairs need to be explored."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
